{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    EncoderDecoderConfig,\n",
    "    EncoderDecoderModel,\n",
    "    BertTokenizerFast,\n",
    ")\n",
    "\n",
    "from models.baseline import Seq2Seq, Encoder, Decoder\n",
    "from data_utils.dataset import TranslationDataset\n",
    "from data_utils.lang import read_langs, PAD\n",
    "from pl_utils.pl_model import ModelWrapper\n",
    "from pl_utils.pl_dataset import PlTranslationDataset\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "TEST_SHARE = 0.2\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_path = \"ai-forever/ruBert-base\"\n",
    "encoder_path = \"bert-base-uncased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(encoder_path)\n",
    "dec_tokenizer = BertTokenizerFast.from_pretrained(decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = AutoConfig.from_pretrained(encoder_path)\n",
    "decoder_config = AutoConfig.from_pretrained(decoder_path)\n",
    "\n",
    "model_config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n",
    "model = EncoderDecoderModel(model_config) # .to(DEVICE)\n",
    "\n",
    "model.config.decoder_start_token_id = dec_tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data.txt\", 'r') as flines:\n",
    "#     all_lines = np.array(flines.readlines())\n",
    "\n",
    "_, _, pairs = read_langs(\"en\", \"ru\", \"data.txt\", False)\n",
    "\n",
    "test_size = int(TEST_SHARE * len(pairs))\n",
    "train_size = len(pairs) - test_size\n",
    "\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=TEST_SHARE, random_state=42)\n",
    "val_pairs, test_pairs = train_test_split(val_pairs, test_size=TEST_SHARE, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(pd.DataFrame(data=train_pairs))\n",
    "val_dataset = Dataset.from_pandas(pd.DataFrame(data=val_pairs))\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame(data=test_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"ru\"\n",
    "\n",
    "def preprocess_function(\n",
    "        pairs: Dataset,\n",
    "        source_lang: str = \"en\",\n",
    "        target_lang: str = \"ru\",\n",
    "        enc_tokenizer=tokenizer,\n",
    "        dec_tokenizer=dec_tokenizer,\n",
    "    ):\n",
    "    inputs = pairs[source_lang]\n",
    "    targets = pairs[target_lang]\n",
    "\n",
    "    max_input_length = int(np.percentile([len(s) for s in inputs], 95))\n",
    "    max_target_length = int(np.percentile([len(s) for s in targets], 95))\n",
    "    max_length = max(max_input_length, max_target_length)\n",
    "\n",
    "    model_inputs = enc_tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = dec_tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"attention_mask\"] = labels[\"attention_mask\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "return_cols = ['input_ids', 'attention_mask', 'labels']\n",
    "\n",
    "tok_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tok_train_dataset.set_format(columns=return_cols)\n",
    "tok_val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "tok_val_dataset.set_format(columns=return_cols)\n",
    "tok_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "tok_test_dataset.set_format(type=\"torch\", columns=return_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = dec_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, dec_tokenizer.pad_token_id)\n",
    "    decoded_labels = dec_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    try:\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        result = {\"bleu_hf\": result[\"score\"]}\n",
    "    except ZeroDivisionError:\n",
    "        result = {\"bleu_hf\": 0.}\n",
    "\n",
    "    result[\"bleu_nltk\"] = corpus_bleu(decoded_labels, decoded_preds)\n",
    "\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "model_name = \"bert2bert\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=1500,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tok_train_dataset,\n",
    "    eval_dataset=tok_val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 1:46:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu Hf</th>\n",
       "      <th>Bleu Nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.549300</td>\n",
       "      <td>0.521950</td>\n",
       "      <td>4.974100</td>\n",
       "      <td>0.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>0.450316</td>\n",
       "      <td>6.398000</td>\n",
       "      <td>0.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.429100</td>\n",
       "      <td>0.416468</td>\n",
       "      <td>7.009500</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.395600</td>\n",
       "      <td>0.393089</td>\n",
       "      <td>7.781500</td>\n",
       "      <td>0.284600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.378003</td>\n",
       "      <td>8.645100</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.367079</td>\n",
       "      <td>9.450000</td>\n",
       "      <td>0.325600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.357557</td>\n",
       "      <td>10.085600</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.351553</td>\n",
       "      <td>10.475800</td>\n",
       "      <td>0.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.339500</td>\n",
       "      <td>0.347449</td>\n",
       "      <td>10.706000</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.345117</td>\n",
       "      <td>10.966400</td>\n",
       "      <td>0.340400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/user-name-goes-here/.cache/pypoetry/virtualenvs/ml-mipt-Iytp8GaD-py3.8/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15000, training_loss=0.5782082529703776, metrics={'train_runtime': 6384.5936, 'train_samples_per_second': 18.795, 'train_steps_per_second': 2.349, 'total_flos': 2.689997540270904e+16, 'train_loss': 0.5782082529703776, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert2bert-finetuned-en-to-ru/checkpoint-15000\"\n",
    "model = EncoderDecoderModel.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def best_worst(model, dataset):\n",
    "    best_ex = []\n",
    "    worst_ex = []\n",
    "    \n",
    "    for i, target in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        if i == 100:\n",
    "            break\n",
    "        kwargs = dataset[i:i+1]\n",
    "        trans = model.generate(**kwargs).squeeze()\n",
    "\n",
    "        translated = dec_tokenizer.decode(trans, skip_special_tokens=True)\n",
    "        target_sent = dec_tokenizer.decode(target['labels'], skip_special_tokens=True)\n",
    "        en_sent = tokenizer.decode(target[\"input_ids\"], skip_special_tokens=True)\n",
    "        score = corpus_bleu([[translated]], [target_sent])\n",
    "        \n",
    "        # считаем по бейзоайну\n",
    "        if score > 0.3:\n",
    "            best_ex.append([en_sent, target_sent, translated, score])\n",
    "        \n",
    "        if score < 0.1:\n",
    "            worst_ex.append([en_sent, target_sent, translated, score])\n",
    "    \n",
    "    return best_ex, worst_ex\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062a82d425a44a9bbcf4d4a52bb3015f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = best_worst(model, tok_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['featuring a hairdryer, the private bathroom also comes with towels and free toiletries.',\n",
       "  'также в распоряжении гостеи собственная ванная комната с феном, полотенцами и бесплатными туалетно - косметическими принадлежностями.',\n",
       "  'в собственнои ваннои комнате с душем предоставляются бесплатные туалетно - косметические принадлежности и бесплатные',\n",
       "  0.5589788093099456],\n",
       " ['all rooms come with clean bedding.',\n",
       "  'все номера укомплектованы свежим постельным бельем.',\n",
       "  'все номера оснащены телевизором с кабельными каналами.',\n",
       "  0.40769919888479106],\n",
       " ['it features free wi - fi and a furnished terrace with swimming pool.',\n",
       "  'к услугам гостеи бесплатныи wifi и меблированная терраса с бассеином.',\n",
       "  'к услугам гостеи открытыи бассеин, терраса и бесплатныи wi -',\n",
       "  0.6572218598665456],\n",
       " ['there is a dining area and a kitchen.',\n",
       "  'в числе удобств — обеденная зона и кухня.',\n",
       "  'в числе удобств — обеденная зона и кухня.',\n",
       "  1.0],\n",
       " ['a full breakfast buffet is served every morning at the clos raymi, which guests can enjoy on the terrace on sunny days.',\n",
       "  'по утрам для гостеи отеля clos raymi накрывается полныи завтрак « шведскии стол ». в солнечные дни трапезу можно провести на террасе.',\n",
       "  'каждое утро в отеле сервируется завтрак « шведскии стол », а в ресторане отеля',\n",
       "  0.33979867483068776]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the hotel is conveniently situated to reach paris bercy stadium ( popb ), vincennes castle and zoo, horse tracks and disneyland resort paris.',\n",
       "  'отель удобно расположен для достижения стадиона paris bercy ( popb ), замка винсенс и зоопарка, конных треков и парижского диснеиленда.',\n",
       "  'отель находится в городе сан - де - де - де - де - де - ла -',\n",
       "  0.07602253426229971],\n",
       " ['the famously narrow punkaharju ridge is 45 minutes ’ drive from pajarinhovi, while the russian border at niirala is a 1 - hour journey.',\n",
       "  'известныи узкии хребет пункахарью расположен в 45 минутах езды от отеля pajarinhovi, а контрольно - пропускнои пункт ниирала на границе с россиеи - в 1 часе езды.',\n",
       "  'отель типа « постель и завтрак » la la la la la las находится в 1,',\n",
       "  0.062415559546089294],\n",
       " ['guest house zolotaya rybka is located in olginka, 48 km from lazarevskoye and 18 km from tuapse.',\n",
       "  'гостевои дом « золотая рыбка » находится в селе ольгинка, в 48 км от микрораиона лазаревское и в 18 км от туапсе.',\n",
       "  'апартаменты « на » расположены в городе сан - де - де - де - де - де',\n",
       "  0.0871250931246731],\n",
       " ['the artwork and decor in each danhostel roskilde guest room reflects various danish historical and cultural themes.',\n",
       "  'декор и произведения искусства каждого номера хостела danhostel roskilde отражают различные исторические и культурные эпохи дании.',\n",
       "  'в отеле la larsa lodge есть гидромассажная ванна и гидромас',\n",
       "  2.3338221488579044e-78],\n",
       " ['situated in the bang rak district in bangkok, 1. 3 km from patpong, the backpack hostel boasts air - conditioned rooms with free wifi.',\n",
       "  'хостел the backpack находится в раионе банграк города бангкок, в 1, 3 км от раиона патпонг. к услугам гостеи номера с кондиционером и бесплатным wi - fi.',\n",
       "  'апартаменты « на » расположены в городе сан - де - де - де - де - де',\n",
       "  0.08973518575208302]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
