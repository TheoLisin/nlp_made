Slides:
[link](https://github.com/girafe-ai/natural-language-processing/blob/22s_made/week05_transformer/MADE__NLP_05_s22_Transformer.pdf)

NMT with attention:
* Completed version:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/natural-language-processing/blob/22s_made/week05_transformer/practice_04_seq2seq_nmt__with_attention.ipynb)

* Positional Encoding carriers:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/natural-language-processing/blob/22s_made/week05_transformer/practice05_positional_encoding_carriers.ipynb)


Further readings:

* Great explanation of attention and seq2seq translation by Lena Voita: https://lena-voita.github.io/nlp_course.html#preview_seq2seq_attn
* Great blog post by Jay Alammar: http://jalammar.github.io/illustrated-transformer/
* Layer Normalization paper: https://arxiv.org/abs/1607.06450
* Blog post explaining Positional Encoding: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/
